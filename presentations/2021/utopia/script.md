Okay! let's talk about my paper, to live in their utopia: why algorithmic systems create absurd outcomes.

This'll take about 5 minutes.

I wrote this paper because I was looking for a theory that draws on power and governance to try to frame a useful way to think and talk about algorithmic systems.

I've written about this kind of stuff before when I talked about street-level algorithms, but that was at the level of individual cases

Street-level algorithms spoke to why AIs make the wrong calls in the moments of decision when they encounter marginal or novel cases.

It didn't really answer the broader structural questions that I still felt hanging around in the air.

So I wanted to try to address that.

What I came up with was this framework that starts from James Scott's *seeing like a state*.

He lays out how administrative states come up with a sort of "bureaucratic imagination" of how the world works by defining how to measure and count stuff.

It's not so much the fact that bureaucracies decide how and what to count that Scott critiques.

Making sense of the world by focusing on limited dimensions is something we all do - Scott talks about these "abridged maps" of the world that we construct.

Sometimes they're literally maps - as in cartographic artifacts - but even more often they're conceptual maps.

Ways of thinking about and making sense of the world.

Anyway, his critique was that sometimes bureaucracies use these abridged maps as blueprints for how the world *should be*, and that can become really dangerous because when you try to go from *navigating* the world to *shaping* the world with a flawed map, you run into (and cause) all sorts of issues where reality doesn't match the "bureaucratic imagination" of the administrative state.

In the paper I argue that algorithmic systems commit the same kinds of harms.

Machine learning systems construct computational models of the world and then impose them on us, not just navigating the world with a shoddy map, but actively transforming the world with it.

They're shaping the world by deciding whether an applicant gets a loan, or a job, or into a good school; who we get matched up with in dating apps; whether the content we put online immediately gets flagged for review or demonetized; whether we get released on bail or held on remand until our court date.

And all of their behaviors in turn shape ours, so that we appear more legibly to this incredibly limited system.

These systems become more actively dangerous when they go from "making sense of the world" to "making the world make sense"; when we take all of this data and tell a machine learning system to produce a model that rationalizes all of that data.

The rules machine learning systems infer from the data have no underlying meaning or reason behind them.

They're just patterns, without any insight into *why* Black people are in prison at much higher rates than white people (for instance).

There's no dataset in the world that adequately conveys white supremacy, slavery, and colonialism.

So at best these systems generate a facsimile of a world with the shadows of history cast on the ground - skewed, flattened, and always lacking depth that only living these experiences can bring.

These rules are devoid of meaning, but in the world the AI has constructed, everything makes a kind of sense.

And they punish or reward us for fitting into the model they generate - in other words, the world they construct.

This is where I start borrowing from David Graeber's *utopia of rules*, where he talks about the utopia that bureaucracies enjoy by imagining a world where everything makes sense according to their rules.

Whether that world even resembles the one we live in is where Graeber takes things next.

He argues that bureaucracies that do absurd things but have no particular power over people have to self-correct (or be corrected) because they have no place in a world where people can freely reject or exit the bureaucracy's nonsense.

But here's the thing: when an institution towers over people, and they can't just leave anymore, those institutions can (and do) get more and more detached from the lives and needs of people.

Those bureaucracies construct their own worlds where everything gets "rationalized" in simplified, reductive language.

These worlds don't have any framework for things like race, or gender, or any of the aspects of our lives that meaningfully inform our experiences.

For those of us who can just *not* deal with race, or gender, or sexuality, we get to pass through these systems relatively unscathed.

But for those of us who can't ignore those dimensions of who we are, those aspects of ourselves make us stick out.

We have dimensionality to ourselves that we can't abandon - that we shouldn't have to abandon.

That depth challenges the model in many ways.

This is different from the conversations we have about bias in data or code.

I think those things are important, but a lot of the conversations about those issues often seem to fade away once the system is out in the world; people talk about "debiasing" data and reviewing code before a model is trained and deployed.

What I'm saying is that even if you've done everything right, if you don't pay attention to the power dynamics as they unfold and play out, the system out in the world is going to drift further and further away from reality.

And that system will make escalating demands of people who have no choice but to try to appease this absurd system.

I end the paper with some design recommendations, mostly falling out from the spirit of Graeber's work, which is to say that we shouldn't allow these systems to have the kind of power that allows them to derail a person's life.

We should work to disempower algorithmic systems wherever we can, and do everything we can to help people escape these systems when they feel mistreated by it.

In some cases that'll mean advocating for human review mechanisms; in others, it'll call for the abolition of the technology and refusal to participate in building the technology itself.

Regardless of the particular tactics, we have to *constantly* be paying attention to the power dynamics between the systems we deploy and the people that system acts upon.

If we're not careful, it'll try to force everyone to live in the algorithmic imagination it constructed - to live in their utopia.

Anyway, that's the paper in a few minutes.

If you have any questions, feel free to tweet at me or whatever.

I'd love to hear if this talk prompted any thoughts or ideas that you'd like to share.

Feel free to tweet at me or email me - my contact information should be on the screen, or just look me up.

Thanks for watching