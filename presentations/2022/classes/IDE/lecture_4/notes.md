so the promise we always hear about AI is that all of the data that we feed into them make them know more about the world than we can, that they have some kind of insight into the world that rivals ours.

in some, very specific ways, that's fair. but it's important to understand how quickly the aspects of our lives that we don't encode into these systems make them not just not "understand" things as well as us, but make them *grossly* inappropriate.

we see examples like computer vision systems that label people of color as subhuman. why would a slur be known to algorithmic systems? why would any aspect of that dimension exist? it seems preposterous. but to be sensitive to the kind of harm that's possible through ignorance or incompetence, we have to be knowledgeable about a wide range of things, and we have to be attentive to people when they try to tell us that an entire dimension of their existence doesn't seem to appear in our models and datasets.

and there were a lot of good comments on all of these thoughts.

Waze routes people into treacherous environments because the data used to train the model evidently wasn't narrowly attuned to the kinds of things that would be necessary for routing algorithms to work: things like some kind of awareness or sensitivity to climate, and to the condition of the road, and to in-the-moment circumstances. as some of you said, you rely on these technologies a great deal for things like navigation; as you go further and further into places that you're not familiar with, or as your work (like as a gig worker) requires you to follow the instructions of the routing system more and more carefully, how increasingly consequential do these systems become?



# notes:
- "weather was not a parameter"
- didn't account for extreme weather conditions
- "dependent on the quality of the question initially asked. Is this however unique to AI? Is this not the case with research in general?"
- *excessive optimism*
- surprising that **misspellings** weren't considered
- concrete examples of language models?
- seasonality and temporary issues in models


# provocations
- how could we get ahead of the problem, especially if our job is to build models working towards these goals?
- what ways do you typically do with a project after you've "finished" building it?
- "Was the testing software supposed to collect data on race?"