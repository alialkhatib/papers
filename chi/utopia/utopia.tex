\documentclass[manuscript,screen]{acmart}

\usepackage{subfiles,xcolor}
\usepackage[super]{nth}
\usepackage[inline]{enumitem}



\copyrightyear{2021}
\acmYear{2021}
\setcopyright{acmlicensed}\acmConference[CHI '21]{CHI Conference on Human Factors in Computing Systems}{May 8--13, 2021}{Yokohama, Japan}
\acmBooktitle{CHI Conference on Human Factors in Computing Systems (CHI '21), May 8--13, 2021, Yokohama, Japan}
\acmPrice{15.00}
\acmDOI{10.1145/3411764.3445740}
\acmISBN{978-1-4503-8096-6/21/05}


\newcommand{\onlyinsubfile}[1]{#1}
\newcommand{\ali}[1]{{\color{gray}{#1}}}
\newcommand{\tocite}[1]{\textbf{\textsc{\color{red}{[#1]}}}}
\newcommand{\topic}[1]{{\color{blue}{#1}}}
\newcommand{\notinsubfile}[1]{}
\begin{document}
\renewcommand{\notinsubfile}[1]{#1}
\renewcommand{\onlyinsubfile}[1]{}
\renewcommand{\topic}[1]{#1}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[To Live in Their Utopia]{To Live in Their Utopia: Why Algorithmic Systems Create Absurd Outcomes}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ali Alkhatib}
\email{hi@al2.in}
\orcid{https://orcid.org/0000-0003-0750-2807}
\affiliation{%
  \institution{Center for Applied Data Ethics, University of San Francisco}
  \streetaddress{101 Howard St}
  \city{San Francisco}
  \state{California}
  \postcode{94102}
}


\renewcommand{\shortauthors}{Alkhatib}

\begin{abstract}
  The promise AI's proponents have made for decades is one in which our needs are predicted, anticipated, and met - often before we even realize it.
  Instead, algorithmic systems, particularly AIs trained on large datasets and deployed to massive scales, seem to keep making the wrong decisions, causing harm and rewarding absurd outcomes.
  Attempts to make sense of why AIs make wrong calls in the moment explain the instances of errors,
  but how the environment surrounding these systems precipitate those instances remains murky.
  This paper draws from anthropological work on bureaucracies, states, and power,
  translating these ideas into a theory describing the structural tendency for powerful algorithmic systems to cause tremendous harm.
  I show how administrative models and projections of the world create marginalization, just as algorithmic models cause representational and allocative harm.
  This paper concludes with a recommendation to avoid the absurdity algorithmic systems produce by denying them power.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003126</concept_id>
       <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}
\keywords{HCI, Artificial Intelligence, Street-Level Algorithms}
\maketitle



\section{Introduction}

HCI researchers have spent years working to improve algorithmic systems, and increasingly systems that produce computational models generated by Machine Learning (ML), that designers often use at enormous scales to classify and make difficult decisions for us.
Some of that work is exploratory, finding new places and ways to use technologies, and new insights that AI might yield when ML is applied to massive datasets to find relationships in the data
\cite{10.1145/3170427.3174367,10.1145/3290607.3312948,Yang:2019:UAF:3290605.3300468}.
Other work surfaces problems with existing systems and attempts to mitigate those harms (for instance, by making them more fair, accountable, and transparent)
\cite{Amershi:2019:GHI:3290605.3300233,Jakesch:2019:ACP:3290605.3300469,10.1145/3290605.3300863,Kocielnik:2019:YAI:3290605.3300641,10.1145/3313831.3376590}.
Then there's work that tries to establish productive theoretical frameworks describing the social environments these systems produce and that designers create and foster, in the hope that some ontology or paradigm will motivate theoretically-grounded discussions about where the first two threads of research ought to lead
\cite{Eslami:2018:CAP:3173574.3174006,Eslami:2015:IAA:2702123.2702556,Eslami:2019:UAT:3290605.3300724,10.1145/2559206.2578883,10.1145/2998181.2998321,10.1145/3173574.3173590}.


Part of the challenge of all this seems to be that the future we've imagined and promoted for decades, as designers of technical systems, is woefully misaligned from people's experiences of massive computational systems.
Many of these algorithmic systems, especially ML systems, cause substantial harms in myriad domains, often surprising the designers of those systems.


Designers of sociotechnical systems have repeatedly built computational systems and models rendering decisions that exacerbate and reinforce historical prejudices, oppression, and marginalization.
As designers of systems, our research community creates and promotes classification systems that guess someone's personality traits based on eye movements \cite{Berkovsky:2019:DPT:3290605.3300451};
what we will come to acknowledge only later is how these systems pathologize neurodivergence, and subject neuroatypical people to even more scrutiny and harm than they experience in society
\cite{doi:10.1350/ijps.2010.12.2.174}.
Researchers design work management algorithms that determine whether and how much to pay workers and whom to assign work
\cite{Cai:2016:CRI:2858036.2858237};
 what we've found since then is that these systems marginalize workers and diminish autonomy for people whose employment precarity was already a looming factor of their lives
\cite{10.1145/2384616.2384663,10.1145/3148148,10.1145/2145204.2145354,takingAHITMcInnis,10.1145/2928269}.
AI researchers propose and advocate for the use of algorithmic systems to make bail recommendations for defendants in criminal trial;
what researchers of the impacts of algorithmic systems later find is that the recommendations these systems produce uncritically learn from and perpetuate the criminalization of Blackness
\cite{propublica,corbett2017algorithmic,smiley2016brute}.
In all of these cases, ML systems surface patterns in the data, generating models that reward recognizable expressions, identities, and behaviors.
And, quite often, they punish new cases and expressions of intersectionality, and marginalized groups.

Some past work has explored why AIs trained on extant data make mistakes at the margins and why they tend to lag behind society by comparing ``street-level algorithms'' to street-level bureaucracies, who \citeauthor{lipsky1980street} wrote are important because they (can) exercise discretion to account for new or extenuating circumstances
\cite{streetLevelAlgorithms,lipsky1980street}.
But that work deals with novel cases, such as when societal norms on the issues most salient to that case have changed.
How scholars of history and justice think about the carceral state is not satisfyingly represented in the historic data (predicated on white supremacy) that researchers and practitioners use to train ML systems and produce models informing bail recommendations.
Our vocabularies surrounding gender and separately sexuality
% (namely, as distinct concepts from one another)
are not adequately codified in the data that engineers use to train monetization systems that conflate the two
\cite{streetLevelAlgorithms}.
In the administrative world, this historical conflation led to the judgment over whether gender identity was intended as a protected class when the Civil Rights Act of 1964 referenced sex.
On the algorithmic side, this conflation manifests when trans YouTubers' videos are demonetized, from all appearances mistaking discussions about gender for discussions about sex and sexuality, as well as myriad other sensitive topics
\cite{tesfaiyoutube,pottinger2018don,doi:10.1177/2056305120936636}.
That lens brings into focus the particularities of these failures, but doesn't exactly explain why algorithmic systems seem structurally tilted toward marginalizing errors.
In other words, ``street-level algorithms'' gives some vocabulary for why this particular failure of YouTube's demonetization system occurs, but doesn't directly answer why the groups that face this extra scrutiny tend to be out and specifically vocal LGBTQ YouTubers and other people from marginalized groups.

This paper centers power as one of the factors designers need to identify and struggle with, alongside the ongoing conversations about biases in data and code, to understand why algorithmic systems tend to become inaccurate, absurd, harmful, and oppressive.
This paper frames the massive algorithmic systems that harm marginalized groups as functionally similar to massive, sprawling administrative states that James \citeauthor{scott1998seeing} describes in \emph{Seeing Like a State}
\cite{scott1998seeing}.
Importing this framework allows us to think about the ``bureaucratic imaginations'' that anthropologists studying power have written about for decades - how bureaucracies see (or fail to see) the world, in contrast to the world that is lived and experienced by people
\cite{nuimeprn1123}.

In creating and advancing algorithmic recommendation and decision-making systems, designers consolidate and ossify power, granting it to AIs under the false belief that they \emph{necessarily} make better or even more informed decisions.
Part of the harm surfaces when the data or the code, or both, go uninterrogated before and during deployment.
But part of the harm emerges later, as these systems impose the worldview instantiated by the ``abridged maps'' of the world they generate;
when they  assume ``a flattened, idealized geography where frictions do not exist''
\cite{GoJekAlgo}, and crucially when those maps cannot be challenged by the people most affected by the abridgments systems make.

This inability to push back against flawed computational models dooms even well-intentioned systems.
Damage manifests most profoundly not only when errors get made, but when people are compelled to endure those errors.
This paper will show that absurdity follows when
algorithmic systems deny the people they mistreat the status to lodge complaints, let alone the power to repair, resist, or escape the world that these systems create.
In other words, when algorithmic systems try to insist that they live in their utopias.
% The ability to tell the navigation algorithm that the directions are unreasonable, or to tell a work assignment system that you're not as near the destination as it thinks
% \cite{chan2018mediatization}.

This paper introduces and works through a description of algorithmic systems framed as administrative states, empowered to threaten and inflict harms.
This framing allowed us to borrow some of the vocabulary that anthropologists had reserved for bureaucracies, like ``legibility'' and ``ambiguity'' from \citeauthor{scott1998seeing}, and ``absurdity'' from \citeauthor{graeber2015utopia}
\cite{graeber2015utopia,scott1998seeing}.
With this vocabulary, we'll dig into some of the questions that ``street-level algorithms'' opens up, in an attempt to describe a structural tendency for algorithmic systems that learn from data and instantiate and motivate objectives that specifically misread marginalized groups disempowered by the historical social structures of power surrounding the systems that cause harm.

Anthropological theories about states, power, and administrative violence can also give researchers tools to work through how designers and participants in systems can avoid catastrophic harms in the first place.
As we'll come to find, both \citeauthor{scott1998seeing} and \citeauthor{graeber2015utopia} offer their thoughts here, taking different approaches but essentially seeing the same origin of the problem - that absurdity and tragedy tend to manifest when bureaucratic imaginations diverge from reality and when people can't override the delusions baked into those imaginations.
In other words, when the overwhelming powers of these states overcome people, those people and others suffer - and the state (or, in this case, the model) gets a little bit further from reality.

I'll end the paper arguing that goals like fairness, accountability, and transparency are themselves merely constructions in the struggle against oppressive authoritarianism, in that these are discursive tools to formalize how we talk about harms, rather than work to dismantle stratified and oppressive power itself, borrowing in large part from \citeauthor{graeber2015utopia}'s anarchist tendencies
\cite{graeber2015utopia}.
I'll attempt to make the case that our community as researchers, as designers, and crucially as participants in society should focus our efforts on designing and supporting resistance \& escape from these domains.
In doing so, I hope to lend support to the growing movement for resistance and refusal, advocating to build (if \emph{anything}, if \emph{ever}) technologies to undermine the ``algorithmic imaginations'' that otherwise threaten, coerce, and punish marginalized people
\cite{feministManifestNo}.

\section{Background}

To illustrate how administrative imaginations profoundly transform the social and natural world, \citeauthor{scott1998seeing} outlines a case of literal natural ecological transformation, in his detailing of scientific foresters during the mid to late-1700s.
He writes of how scientific foresters sought to transform the world into logical, reasonable components.
They constructed what \citeauthor{scott1998seeing} called ``abridged maps'' - maps that made administrating the forest both conceptually and literally more easily.
These maps were important, but not the singular causes, of the ``full-fledged disasters'' that motivate his analysis.
What \citeauthor{scott1998seeing} argues of states, and I argue of AI, is that four factors together result in substantial harms and disasters: 
\begin{enumerate*}
\item administrative transformational reordering of nature and society;


\item ``high-modernist ideology'' - the unscientific optimism about the capacity for comprehensive planning to improve life;


\item a ruthlessly authoritarian state with disregard for human life, especially to advance the modernist's cause;
and finally 

\item a weakened civil society.
\end{enumerate*}
In this section, I'll recount how that came to pass in the near-total deforestation of Europe, serving as a guide for the abridgment and authoritarianism taking place in sociotechnical systems, particularly (but not exclusively) those that rely on machine learning models.

\subsection{Seeing (and Transforming) the Forest for the Trees}

At the turn of the \nth{18} century, people in Europe were desperate for timber.
Harvesting vast tracts of forests to meet a demand that seemingly couldn't be satisfied, after some time people began to realize that there was no way this could continue sustainably.
So people became experts at estimating how much timber a forest could yield without irreparably harming the forest, starting by doing things like figuring out a tree's yield from various measurements, or sampling a plot of a forest to infer the whole forest's capacity.

A field called ``scientific forestry'' emerged, and its practitioners thought about how to optimize forests to sustainably maximize productivity.
These people viewed the forest in a radically different way from the past: they regarded and thought about the forests as a natural resource to be exploited in a newly systematic way, using a sort of ``fiscal forestry'' as \citeauthor{scott1998seeing} called it, and thinking about these resources exclusively in the terms of the applications that they could yield: products that could be made with them, taxes that could be levied, and profits to be made.
What was legible to them were things that could be sold as raw natural resources, or even as processed products.
A school of thought took hold - one that called for a whole system of maintaining forests with careful, absolute precision: they removed the underbrush, the fallen leaves, the fungus, the wildlife;
they removed and replanted the trees in a literal grid, like a pristine garden, with a single species of tree that they found was most productive.
The ecosystem became a monoculture.

\citeauthor{scott1998seeing} writes that, for the first several decades, scientific foresters enjoyed unprecedented success and bounty.
But after 70 or 80 years - that is, after a generation of trees had run its full course - things started to go terribly wrong.
Pests that thrived on Norway Spruce (the tree species that had been selected because it yielded the greatest volume of lumber) overwhelmed the forest, making thick clouds of insects through which people could hardly see.
And all of the things that contribute to soil production - birds, insects, and animals, for instance - had no place to go when foresters aggressively managed them out.
All of the trees being the same approximate age also made them all similarly vulnerable when storms came through - there were no larger trees to protect the smaller, younger trees.
Something like 20-30\% of the forest's productivity vanished in the second generation, with some doing far worse;
a word entered the German vocabulary around that time: ``waldsterben'', or ``forest death''.

\citeauthor{scott1998seeing} describes what went wrong in this and in other ambitious schemes: It was one thing when people who engaged in scientific forestry simplified the world as a means to interpret it;
or put another way, the maps themselves were not exactly the problem.
The critique he makes clear is the ``apparent power of maps to transform as well as merely to summarize the facts that they portray \dots invented and applied by the most powerful institution in society: the state''.
But all that was mediated by the extent to which the institution was responsive to the reality upon which it tried to project that vocabulary and grammar - that ``bureaucratic imagination''
\cite{nuimeprn1123} - back onto the world.
When states construct abridged versions of the world that don't conform to the world, those maps can be tremendously harmful as analytical tools that mislead us.
More than that, when modelers and designers of influential systems use these maps as guides to substantially transform the world, the abridgments and the omissions they make become targets of erasure.

The conceptual abridged maps scientific foresters created and used are artifacts that summarize the world, but they transform the world as well.
For one thing, they express a worldview that motivates how people relate to the natural world, in this case the forest.
But as I pointed to earlier, this is just the first of four characteristics that precipitate disaster.
What remains is the belief that science and technology will lead us ``to manage and transform [society] with a view toward perfecting it'' (high modernist ideology);
the willingness to subjugate and harm others - often dismissed as errata and anomalies - to achieve that vision;
and the weakening of civil society.

\subsection{Transforming Information into Data}

\citeauthor{bowker2000sorting} wrote some of the foundational work critically interrogating what gets measured and how, and how those measurements shape and change the lives of people.
In their description of the sometimes-tense negotiations between doctors, insurers, and patients, they offer a careful look into the kinds of translations that have to occur to achieve one's goals in the medical and insurance administrations:
``a physician decides to diagnose a patient using the categories that the insurance company will accept.
The patient then self-describes, using that label to get consistent help from the next practitioner seen.
The next practitioner accepts this as part of the patient's history of illness''
\cite{bowker2000sorting}.
They drive this point further, showing that the constructions medical professionals use in medicine and insurance influence patients' ``biographies'' or lived experiences, those lives ``torqued'' in the intersections of ``dichotomous classification schemes''
\cite{bowker2000sorting}.

As in \citeauthor{scott1998seeing}'s work, this paper's central point isn't that our systematized classification and quantification of the world acts as an interpretive and transformational force;
\citeauthor{bowker2000sorting} make that point \cite{bowker2000sorting}, as do later scholars in domains such as personal care
\cite{10.1145/3274362} and worker management
\cite{turkopticon}.
These forms of violence have taken and continue to take myriad forms, as \citeauthor{hoffmann2020terms} writes of Nazi programs of genocide and ``The New Jim Code''
\cite{hoffmann2020terms}.
The undercurrent of this idea, the administrative and computational re-ordering of our lives and the harms those re-orderings cause, is the first of four characteristics \citeauthor{scott1998seeing} touched upon earlier.
% This signal has been present, and researchers have been observant of it, for decades.
What this paper hopes to show is that a development has happened in the past several years that's bringing the other three factors in convergence with this familiar one, precipitating disasters.

\section{The Worlds That Algorithms Construct}

The models AI researchers train and deploy construct worlds without space for the dimensions of our lives that make us who we are, like race and gender.
For the model that the algorithms generate, data correlating to these dimensions might arise in conspicuous observations from the trends, but the data themselves won't directly implicate the complex histories of slavery, white supremacy, redlining, and other factors that bear on relative inequities in health and the relative inequities of wealth, for residents of the same city.
As a result, the explanations that bear most directly on disparities will continue to elude these systems.
For the people who enjoy privilege in those dimensions like race or gender, the omission of race from an algorithmic model will be inconsequential, maybe even seamless, feeling like the ``default''
\cite{sweeney2019technically}.
But other people have to recognize that race, gender, their experience of disability, or other dimensions of their lives inextricably affect how they experience the world.
The model doesn't have a place for these people, and the algorithm will flag them as anomalies in relation to the model based on data that never adequately represented them.

To the machine learning systems developing models of activity, of health, and of financial status, there is no clear answer to why these things happen - their data are ambiguous and confusing, and so they appear anomalously.
The model may have no way of answering why someone might have a disability and be unable to register steps the way training data led the model to predict;
it may have no way of imagining that a woman might have had a miscarriage, or a medical condition making the sudden weight loss a cause of serious concern rather than an achievement
\cite{wachter2017technically};
it may have no way of understanding that race and history are major factors behind wealth inequalities so steep that the median net worth of Black residents in Boston would be \$8, compared to \$247,500 for white residents
\cite{BostonEIGHT}.

Machine learning systems generate models from the data provided, often by the designers of those systems;
but there's more happening behind the scenes.
In the process of training a model, the algorithm creates its own world  - it generates its own sort of \emph{utopia} where things are clear and calculable, and where a sufficiently complex analysis can uncover patterns and correlations;
this is the first quality of the four that \citeauthor{scott1998seeing} warned about.
That system imposes its model upon the world, judging and punishing people who don't fit the model that the algorithm produced in the interest of some ostensibly objective goal that designers insist is better than decisions humans make in some or many ways;
this is the second quality.
Their disregard for people's dignity and lives - or rather, its inability to conceptualize those ideas in the first place - ultimately make them as willing as any system to subjugate and harm people;
this is the third quality.
Finally, our practices in how ethicists and other scholars talk about ethics and AI, undermining and gatekeeping discourse until the public accept that rigorous engagement with ``the algorithm'' is something only philosophers and computer scientists can do;
this is the fourth and final quality.

Framing AI as creating and imposing its own utopia against which people are judged is deliberately suggestive.
The intention is to square us as designers and participants in systems against the reality that the world that computer scientists have captured in data is one that surveils, scrutinizes, and excludes the very groups that it most badly misreads.
It squares us against the fact that the people we subject these systems to repeatedly endure abuse, harassment, and real violence precisely because they fall outside the paradigmatic model that the state - and now the algorithm - has constructed to describe the world.

\subsection{Vignettes}

On the ground, the places we find algorithmic systems causing the most egregious harms tend to exhibit many or all of the four characteristics \citeauthor{scott1998seeing} warned of: 
\begin{enumerate*}

\item administrative transformational reordering;


\item ``high-modernist ideology'';


\item ruthless authoritarian tendencies without regard for human life;
and 

\item a weakened civil society.
\end{enumerate*}
The extent to which people recognize the transformational reordering of the world depends substantially on whose life is being transformed and upended, and whether those people have the power to reject or escape harmful transformations.
In the following vignettes, we'll explore how the misapprehensions ML systems learn from data produce a sort of ``algorithmic imagination'' that makes consequential abridgments in the spaces where people without power are situated.
Without power to resist these systems, the models of these spaces become increasingly detached from the reality the people experience, and increasingly harmful toward those people:

\subsubsection{Police misidentification}
Computer vision systems are generally trained on anywhere between thousands and millions of images, but in any case the models that they construct represent algorithmic imaginations of how the world works according to the system that generated the model, as informed by the images it was provided.
So when a person of color appears in the system - a system constructed without regard for or awareness of the biases inherent to photography itself
\cite{benjamin2019race,photographySkin}
 - that person experiences the friction of trying to exist in this system's world with a dimension that the system does not recognize.
 As a result, computer vision systems make embarrassing misidentifications like those labeling members of congress as suspects of past criminal cases
\cite{membersOfCongress}, but the trends of those errors reveal the underlying biases that pervade all other misidentifications - specifically that these misidentifications are so often of people of color, and often of Black men
\cite{clearviewArrest}.
These systems create models of the world that totally fail to account for the history of criminalizing Blackness
\cite{smiley2016brute},
not just in the form of ``Black codes, vagabond laws, and prison labor camps'' that emerged after the Civil War, but in the carceral state's expansion beginning in the 1970s
\cite{bell2017criminalization}.
Without these contexts, computational models have none of the tools necessary to account for biases of which these systems aren't aware in the first place.
The power dynamics of law enforcement in the United States are such that police can arrest, abuse, and even kill Black people with relative impunity, to say nothing of recklessly and cavalierly using a face-matching technology's results which are not only often wrong with people of color, but get even worse when more dimensions, such as gender, get layered on
\cite{buolamwini2018gender}.
Further, the secrecy surrounding police procurement of these technologies limit public engagement to those already in positions of political and computational power.

\subsubsection{Body Scanning and Gender Identity}
\citeauthor{costanza2018design} has written of their experience traveling through airport security, subjected to the millimeter-wave scanning technology that images travelers and returns an algorithmic assessment of the person being imaged
\cite{costanza2018design}.
When these systems flag their body and the bodies of other trans people as ``anomalous'',  we all observe the world that the computer vision system has constructed: a world without a concept of gender identity beyond ``male'' or ``female'', to say nothing of trans identity.
This system doesn't understand that it reaffirms harmful gender binaries because it can't understand anything.
At its core, this system has constructed a world of rules that essentially works for cisgender men and women.
Theirs is a world of rules that reaffirm binary gender expressions that conform to their sex assigned at birth, through which cisgender people can pass relatively painlessly.
The millimeter wave scanner's computer vision system offers relief, but only if someone can conform to the model of the world that it has constructed -
one without anomalous cases or ambiguous identities.
This is an absurd demand, but it is the one the system makes, and it is why trans people suffer.
Transportation security enforcement in the United States all but ensures that travelers must submit even to humiliating and debasing mistreatment.
And the effective relegation of discussions about these scanners to ``national security'' and other exclusive domains prevents people from meaningfully engaging with the system, even when they wish to correct it, for instance by attempting to re-train the model to recognize that trans and non-binary people exist, how they may present, and that they should not be regarded with suspicion.

\subsubsection{Algorithmic Judgment in Academia}
In 2013, the Computer Science department at the University of Texas at Austin developed a machine learning system, named ``GRADE'', which they used to inform graduate admissions decisions.
GRADE valued words on students' applications like ``best'', ``award'', and ``research'', and devalued words like ``good'' and ``class''.
The system assigned zero weight to ethnicity, gender, and national origin, remarked upon by the authors of the paper describing GRADE as suggestive proof that ``admissions decisions are based on academic merit''
\cite{waters2014grade}.
More than 5 years after its implementation, the department announced that it had phased out use of GRADE, realizing that the system had produced recommendations skewed against women and people of color
\cite{UTCSBias}.
For graduate student applicants, for whom this system largely decided the trajectory of their adult lives, these systems were tremendously consequential;
for the designers of the system, what mattered was that the system
``reducing the total time spent on reviews by at least 74\%''
\cite{waters2014grade};
the system was never trained to regard or even perceive diversity, but instead to operationalize the amount of work that admissions committees had to do in selecting whom to admit.
This system didn't seek to undermine women or people of color, but in constructing a world with no way of accounting for gender or race, and therefore no way to make sense of the unique challenges applicants had to overcome just to be even vaguely legible data points, what the system ultimately asked of applicants was to somehow shed the experiences relating to race and gender that shaped who they are in order to be legible within the utopia that the system has constructed.
This demand is something that, for instance, white male applicants can acquiesce to, because society makes space for whiteness and for men as the ``default''.
But these experiences are all but impossible for marginalized groups to set aside.
And despite the bearing this system had on their futures, at no point did prospective or even current students seem to have input into how the system evaluated applicants (except as data points themselves), to say nothing of whether they consented to being evaluated in this way.

\subsubsection{GPS Navigation and On-Demand Workers}
Navigation systems enable on-demand delivery and driving services around the world for many workers who would not have been able to do that work before
\cite{pieceworkCrowdworkGigwork}.
But these systems are transformative in themselves, creating ``premediated socio-spatial relations between drivers and riders'', such as deciding who needs to go where, and even how to get there, regardless of the geography
\cite{chan2018mediatization}.
\citeauthor{GoJekAlgo} writes specifically to this point: that algorithmic navigation systems and work assignment algorithms assume ``a flattened, idealized geography where frictions do not exist, only supply and demand.
In this world, the former appears to move easily through mapped streets to the latter''
\cite{GoJekAlgo}.
These systems don't learn from the politics that may influence how safe it is for someone to navigate through a particular neighborhood, or from the impact that drivers will have on the roads.
These gig work platforms, which regard workers as interchangeable cogs in the broader market system, care very little for the wellbeing or status of individual drivers besides their output capacity
\cite{pieceworkCrowdworkGigwork}, let alone for the lasting harm they may be exhorting drivers to do - and so they are willing to ignore workers' needs and endanger them in the process of assigning work and navigating drivers to their destinations.


\section{From Ambiguity to Absurdity}

Earlier I said that machine learning works by training from the data of shallow depth that designers provide them.
They construct models of the world (their own utopia of ``organized stupidity'', as \citeauthor{graeber2015utopia} called them when writing of bureaucracies), and coerce us to fit into the patterns that they've come to learn.
\citeauthor{graeber2015utopia} writes that ``violence's capacity to allow arbitrary decisions, and thus to avoid the kind of debate, clarification, and renegotiation typical of more egalitarian social relations, is obviously what allows its victims to see procedures created on the basis of violence as stupid or unreasonable''
\cite{graeber2015utopia}.

The power dynamics in all of the above cases are such that the people harmed rarely have insight, let alone input, into the system.
And to the extent that they have input, their input is hardly ever regarded: police mistreat, abuse and murder Black people, regardless of their degree of compliance or other characteristics
\cite{moore2018critical}.
The that power transportation security workers have over travelers is seemingly endless, especially over marginalized groups
\cite{chapablanco2018traveling}.
Graduate school applicants have vanishingly little insight into the admissions processes, and certainly no input into the process itself.
And finally, on-demand workers are so ubiquitously dispossessed and marginalized that workers struggle to find and safely engage in even minor acts of resistance.

If the amount of power people can express over these systems mediates how absurd outcomes get, then we should see less absurdity in settings where stakeholders have influence over the street-level algorithms, and more absurdity where power imbalances are steeper.
When systems and data disempower other stakeholders, we see absurd and in some cases terrible outcomes
\cite{Bopp:2017:DDN:3025453.3025694}.
But in contexts where some amount of resistance can be mounted, and especially where stakeholders are regarded as continuing participants in the ongoing construction of the world that these systems create, we find these systems reigned in by the tendencies described earlier
\cite{doi:10.1177/2053951717718855}.
When the algorithm is inscrutable, outcomes that are hard for all involved to explain, let alone defend, manifest
\cite{streetLevelAlgorithms,LGBTYouTube,ibrahim2010breastfeeding,adpocalypseForbes}.

It would be a fair question to ask why any of this is ``utopia'';
it sounds like hell, or at least like an oppressive nightmare.
For people judged by these computational systems, it is.
Machine learning algorithms create \citeauthor{graeber2015utopia}'s ``ways of organizing stupidity'', but in doing so they create an order to the world that is more ``rational'' by finding patterns in the massive and growing troves of data - although as he points out, ``to say one wishes to create a `rational' social order implies that current social arrangements might as well have been designed by the inhabitants of a lunatic asylum''
\cite{graeber2015utopia}.
But that's the implication of machine learning algorithms, constructing models and then projecting that world onto our actual experiences: they create a utopian world that is ``rational'', thus denying the legitimacy of our own experiences and the premise that our constructions were rational in the first place;
and people are obliged to find ways to conform to the patterns they've found and generated, whether someone fits into their world or not.
This is why AIs cause so much harm - because designers and implementers of these systems empower those systems to create their own worlds, attempting to transform the world as we experience it to erase the dimensions that enrich and inform our lives, and then to ask marginalized groups to shed their races, their genders, their disabilities, their identities.
In other words, AIs cause so much harm because they exhort us \emph{to live in their utopia}.

When designers of these algorithmic systems train computational models that ignore transgender identity, those systems demand that trans people somehow shed an identity they can't;
identities that cisgender people hardly ever bother to regard.
When designers construct systems that parse résumés and curricula vitae, and those systems construct a world that can't perceive the effects of gender and race-based discrimination leading up to that ``moment of decision''
\cite{streetLevelAlgorithms}, those admissions systems demand that applicants exist detached from the identities that have inextricably shaped their lives and their profiles as applicants.
When designers of the indexing and search algorithm systems that \citeauthor{noble2018algorithms} writes about deploy search engines, what we find when we search for ``black girls'' is that these systems have constructed a worldview wherein these keywords imply an objectifying sexuality - an implication that people have to work through and defy, against the calls of the system
\cite{noble2018algorithms}.
When designers produce and impose navigation systems without regard for bodies of water and buildings, those systems demand that couriers literally traverse a world that only exists in the model of the system - not in the world that real humans live and experience
\cite{GoJekAlgo}.

These systems coerce us into behaviors and identities that are less expressive, less creative, and more homogenous in order to optimize for overly simplistic goals and metrics.
Patterns that are novel, but not so novel that a profoundly limited pattern-recognition system would be unable to recognize the novelty.
Researchers have discussed this kind of effect in documenting music streaming services' influence on expressed patterns in music
\cite{spotifyLengths};
what this paper shows is that this kind of homogenizing effect, producing a monoculture by coercing, erasing, and oppressing those who don't already fit the existing pattern, takes place wherever AI wields overbearing power, that these harms consistently and inescapably come down on the groups that have vanishingly little power to rebuke, resist, correct, or escape.
% The absurdity and the harm imposed on others diminishes everyone, in part because our world becomes worse and our notions of egalitarianism and justice become increasingly disconnected from the reality that marginalized people experience.

\section{Discussion}

If our goal is to reform and bolster existing systems to make them less harmful, \citeauthor{scott1998seeing} describes the idea of \emph{metis}, which he translates substantively as the intelligence required to adapt to new and changing circumstances
\cite{scott1998seeing}.
We can explore how to construct computational models to have more \emph{metis} so that they can handle novel situations.
If instead we wished instead to dismantle these systems of disempowerment and oppression, \citeauthor{graeber2015utopia} offers one way to address power imbalances in bureaucracies.
In this section I'll also see if we can adapt his advice to the algorithmic space, given the structural parallels that I've drawn already.

\citeauthor{scott1998seeing}'s description of \emph{metis} provides an intriguing critique of AI's potential.
In it, he describes \emph{metis} as more than constructing any number of ``rules of thumb,'' pushing back to say that ``knowing how and when to apply the rules of thumb in a concrete situation is the essence of \emph{metis}''.
He continues to underscore that ``the practice and experience reflected in \emph{metis} is almost always local,'' but that it amounts to more than the specification of local knowledge and values
\cite{scott1998seeing}.
\citeauthor{streetLevelAlgorithms} similarly conclude their work arguing that ``street-level algorithms are liable to make errors in marginal and novel situations,'' owing to the loop-and-a-half delay that they described, suggesting that algorithmic systems will never quite be able to close this gap
\cite{streetLevelAlgorithms}.

Building less terrible systems would require an insight and understanding of the circumstances surrounding a person's life that people with privilege can hardly fathom, let alone encode.
\citeauthor{benjamin2019race} writes of ``thin description'' in conversation with \citeauthor{jackson2013thin} to point out that the totalizing need for data is one that motivates technology and AI but dooms it
\cite{benjamin2019race}.
\citeauthor{10.1145/3290605.3300528} make a similar point, that a person without the lived experience of disabilities can never truly understand what it means to be ``like'' someone who experiences it (or for that matter many characteristics that overlap and interact with one another), but as designers of systems we should strive to be with those people in designing systems
\cite{10.1145/3290605.3300528}.

\citeauthor{fatBeWilin} similarly critiques the aspiration for ``peer relationships'' in the design of fair, accountable, and transparent algorithmic systems: ``sounds like participatory design \dots except it is not, because of the questions around power, language and positionality''
\cite{fatBeWilin}.
But this is something of the whole point;
that the power dynamics that problematize the idea of ``peer relationships'' would of course make the use of algorithmic systems dangerous and inevitably harmful.
If people are negligent enough in reflecting on their positionality designing these systems, what hope can people reasonably ascribe to an algorithm that can't be reflexive, or have \emph{metis}, in the first place?

As designers, we may instead find it more productive to explore radical approaches, specifically undermining oppressive technologies.
\citeauthor{graeber2015utopia} finishes the penultimate chapter of his book with a reflection on the work that anti-authoritarian and feminist groups engage in to promote and foster activist decision-making processes.
He notes the dilemma \citeauthor{freeman1972tyranny} observes in her essay ``The Tyranny of Structurelessness'': that as groups grow, cliques seem to form that accumulate power and information
\cite{freeman1972tyranny}.
\citeauthor{graeber2015utopia} observes that these groups tend to move toward more transparent structures, in an attempt to make more visible the rules and structures that exist.
He critiques this approach, saying that ``structures of transparency inevitably become structures of stupidity as soon as [formalization of cliques into committees] takes place''
\cite{graeber2015utopia}.

One option \citeauthor{graeber2015utopia} offers is to ``reduce all forms of power to a set of clear and transparent rules''.
I might recognize this value in the FAccT community as well as in HCI
\cite{Amershi:2019:GHI:3290605.3300233,10.1145/3290605.3300295,Eslami:2019:UAT:3290605.3300724,Jakesch:2019:ACP:3290605.3300469,Kocielnik:2019:YAI:3290605.3300641}.
But he cautions against this: ``whatever ‘formal accountability structures' it is imagined will contain the cliques-now-turned-committees can only be far less effective in this regard, not least because they end up legitimating and hence massively increasing the differential access to information which allows some in otherwise egalitarian groups to have greater power to begin with.'' Instead, \citeauthor{graeber2015utopia} advocates that our collective task is to intentionally limit the power that these informal structures have to influence decisions and wield power.
This may seem radical within HCI - that designers should not be designing technologies, or even that designers should be designing to undermine these technologies, but they're not as radical as they might seem.

Researchers in our field and elsewhere already study and think about how people subvert complex sociotechnical systems
\cite{uberAlgorithm,10.1145/3313831.3376424}.
Other work has directly called us to note the shifts of power, rather than ideas of goodness or fairness, in AI - even directly challenging what it means to be ``fair'' or ``transparent'', and to whom
\cite{kalluri2020don}.
In many of these contexts, these works prompt us to think about ways that we can support that subversive act
\cite{doi:10.1177/2053951717718855}.
Work such as the Feminist Data Manifest-No crystallize the value and meaning in saying ``no'', and the importance of refusal
\cite{feministManifestNo,d2020data}.
\citeauthor{10.1145/3313831.3376392} speak to some of the central ideas of this paper -
bringing the practice of design into focus through the lens of critical race theory, and the understanding that constructions such as race and racism are the inventions that come out of specific power structures meant to maintain stratification and to further disempower the ``other''
\cite{10.1145/3313831.3376392}.
Other work points out ``how designers can intervene in power hierarchies by localizing design techniques''
\cite{10.1145/3290605.3300791}, and others still explore when not to participate in this process
\cite{10.1145/1978942.1979275}.
This work brings the ethos of these works to the design of AI, or machine learning systems that classify and judge people at massive scale with the nominal support of being ``data-driven''.
We can and must more carefully reckon with the parts we play in empowering algorithmic systems to create their own models of the world, in allowing those systems to run roughshod over the people they harm, and in excluding and limiting interrogation of the systems that we participate in building.
We have an opportunity to encourage and promote the public in discourse that has led to outcomes like moratoria on facial recognition technologies in various cities \cite{10.1145/3313129}, and in myriad other domains of surveillance, management, and mediation powered by AI \cite{mattern2017city}.

At the heart of this paper is the ethos that many things are unknowable, that ambiguity is an inexorable part of knowing anything, and that this ambiguity and knowledge should be contested.
Some of this comes from \citeauthor{benjamin2019race}'s \emph{Race After Technology}, and more specifically from the work she cites - \citeauthor{jackson2013thin}'s advocacy of ``thin description''
\cite{benjamin2019race,jackson2013thin}.
Or to be more precise, the conscious acceptance that ``thick description'' in the Geertzian tradition is impossible, that it presumes that anyone can recognize everything as ``legible'' as \citeauthor{scott1998seeing} puts it, and document it accordingly;
that ambition to capture everything to comprehensively describe a phenomenon reveals a certain degree of hubris
\cite{benjamin2019race}.
But \citeauthor{jackson2013thin}'s work argues that not everything is legible to any one person;
instead, he argues qualitative researchers should study ``\dots how we all travel through the thicket of time and space\dots '', and accept with humility that we will never be able to fully understand or even document everything
\cite{jackson2013thin}.
This approach, rejecting thick description's attempt to claim complete knowledge of events, says that by admitting that there are things we can't ever know, we can be more flexible in our analysis, allowing us to understand the ``mercurial,'' ``shape-shifting'' practice of the prejudices that we seek to analyze and understand.


\section{Conclusion}

This paper introduced and attempted to work through a description of algorithmic systems framed in some ways functionally as an administrative state, in particular empowered to threaten and inflict harm upon people who don't fit into the ``algorithmic imaginations'' of these systems.
This framing allowed us to borrow some of the vocabulary that anthropologists had developed for bureaucracies, like ``legibility'' \& ``ambiguity'' from \citeauthor{scott1998seeing}, and ``absurdity'' from \citeauthor{graeber2015utopia}
\cite{graeber2015utopia,scott1998seeing}.
With this vocabulary, together we dug into some of the questions that ``street-level algorithms'' opens up, and reveals a structural tendency for algorithmic systems that learn from data and instantiate and motivate objectives that specifically misread marginalized groups disempowered by the structures of power surrounding the systems that cause harm.



\begin{acks}
{More than is usual for me, the process of making sense of these ideas was much more collaborative than the author list implies, for obvious reasons. My unending gratitude to Nana Young, Razvan Amironesei, and Rachel Thomas for their time and thoughts as we worked through earlier versions of some of these ideas. This work also benefited from reviewers who were willing to see through the shortcomings of the initial submission, and went above and beyond to help make this paper the best version of itself that it could be. It would literally not have been possible if not for their extraordinary generosity and care.}
\end{acks}




\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}