two thoughts:

1. i think the question i'm closing in on is "what circumstances or characteristics of AI systems result in outcomes that make people's lives better, versus those that make people's lives worse?". this makes the question about a lot of AI systems directly relevant; the question becomes less about dogmatic prescriptions --- this use of AI is wrong or that one is right --- and more about the context out of which these AI systems emerge. i think it would make it easier for me to look at everything from advertising (and the negative: surveillance state), to bail-setting (and the negative: implicit racism and bias)

2. talking to lilly yesterday and today at the AFOG event that niloufar got me into, lilly got me thinking about her postcolonial computing paper and how it was more about being *suggestive* rather than persuasive --- to be more concrete: it didn't claim to offer a comprehensive model for understanding the world, but asked how this particular lens might help bring certain things into focus, or be a useful way of thinking about specific questions within ICT4D/HCI. i think this is a different objective than the one we've been setting out to do here (or for instance than we did with the piecework paper), but this might free us up to not have all the answers, or rather not require us to tie everything together in a neat bow.





I figure taking a more suggestive stance will do a few things:

+ it'll bring in some feminist HCI scholarship which does similar (that is, present a way of looking at the world that's useful only inasmuch as it's useful, or only in the ways that it's useful)

+ it'll make it easier to move variously between topics in AI fairness and apply reasoning and background selectively rather than try to make the entire body of work fit in front of the other

- it will *not* try to make sense of the world in a broader overarching way
