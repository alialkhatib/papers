%!TEX root = ./street-level_algorithms.tex
\documentclass[street-level_algorithms]{subfiles}

\begin{document}
\section{Introduction}

\topic{People have grown increasingly frustrated with the decisions that algorithmic systems make over their lives.}
These decisions can have weighty consequences:
they determine whether we're excluded from social environments~\cite{gillespie2018custodians,geiger2010work};
they decide whether we should be paid for our work~\cite{takingAHITMcInnis};
they influence whether we're sent to jail or released on bail~\cite{huq2018racial}.
Despite the importance of getting these decisions right,
algorithmic and machine learning systems make surprising and frustrating errors:
they ban good actors from social environments and leave up toxic content~\cite{halfaker2013rise,chancellor2016thyghgapp};
they disproportionately make bail inaccessible for people of color~\cite{propublica};
they engage in wage theft for honest workers~\cite{takingAHITMcInnis}.
Across diverse applications of algorithmic systems,
one aspect seems to come through:
surprise and
--- quite often ---
frustration with these systems over the decisions they make.

\topic{Researchers have approached these problems in algorithmic systems from a number of perspectives:
some have interrogated the hegemonic influence that these systems have over their users~\cite{teachout2014market};
others have documented and called attention to the unfair and opaque decisions these systems can make~\cite{ibrahim2010breastfeeding};
others still have audited algorithms via the criteria of fairness, accountability and transparency~\cite{corbett2018measure}.}
But even if we could answer the questions at the hearts of these research agendas
--- finding clear guiding principles,
    incorporating the needs of myriad stakeholders, and
    ensuring fairness ---
these algorithms will always face cases that are \textit{at the margin}:
outside the situations seen in their training data.
For example, even a value--sensitive~\cite{zhuvalue}, fair, and transparent content moderation algorithm will likely
make errors with high confidence when classifying content with a new slur that it has never seen before.
And it is exactly at these moments that the algorithm has to generalize:
to ``fill in the gaps'' between the policy implied by training data and
a new case the likes of which it has never seen before.

\topic{``Filling in the gaps'' is not a problem unique to algorithms, however:
it has been the chief concern for decades of what bureaucratic theorists call
\textit{street--level bureaucracies}~\cite{lipsky1983street}.}
A street--level bureaucracy is the layer of a bureaucracy that directly interacts with people.
They are responsible for making decisions ``on the street'',
filling in the gaps between legislated policies and the situations in front of them.
Street--level bureaucrats are
police officers, judges, teachers, customer service agents, and others
with whom members of the public interact frequently, and
who make everyday decisions that affect our lives throughout the day.
In all of these roles,
street--level bureaucrats make important decisions about
cases both familiar and new that require them to translate an official policy into decisions about the situations they face.
A police officer chooses whether to issue a warning or a traffic citation;
a judge decides whether to allow a defendant to pay bail or to have them remanded to jail;
a teacher determines whether to waive a course's prerequisites for a student.
These decisions often involve nuance or extenuating circumstances,
making it all but impossible to prescribe the right response for all situations.

\topic{Street--level bureaucracies are important because
the decisions they make are the manifestation of the power of the institution, and are \textit{effectively} policy.}
Regardless of what's explicitly legislated or prescribed,
policy is effected by the street--level bureaucrat's execution of their duties.
Police officers choose whether to issue citations to people speeding only slightly over the speed limit;
judges may make every effort to make bail accessible given a defendant's circumstances;
teachers may allow some flexibility in course prerequisites.
In each case, these street--level decisions become expectations of the system writ large.
When these effective policies are biased
(e.g.,~\cite{Voigt6521}),
it prompts broad critiques and reviews of the policy, organization, or system.


\topic{In this paper,
we draw the analogy to pose people's interactions with algorithmic aspects of sociotechnical systems as
interactions with what we term \textit{street--level algorithms}.}
Street--level algorithms are algorithmic systems that directly interact with and make decisions about people in a sociotechnical system.
They make on--the--ground decisions about human lives and welfare,
filling in the gaps between platform policy and implementation, and
represent the algorithmic layer that mediates interaction between humans and complex computational systems.

\topic{\thesis{Our central claim is that
street--level algorithms make frustrating decisions in many situations where street--level bureaucrats do not, because
street--level bureaucrats can reflexively refine the contours of their decision boundary \textit{before}
making a decision on a novel or marginal case, but
street--level algorithms at best refine these contours only \textit{after}
they make a decision.}}
Our focus here is on cases at the margin: those representing
marginal or under-represented groups, or
others creating novel situations not seen often or at all in the training data.
When street--level bureaucrats encounter a novel or marginal case,
they use that case to refine their understanding of the policy.
When street--level algorithms encounter a novel or marginal case,
they execute their pre--trained classification boundary,
potentially with erroneously high confidence~\cite{attenberg2011beat}.
For a bureaucrat, but not an algorithm, the execution of policy is itself reflexive.
For an algorithm, but not for a bureaucrat, reflexivity can only occur after the system receives feedback or additional training data.
The result is that street--level algorithms sometimes make nonsensical decisions,
never revisiting the decision or the motivating rationale until it has prompted human review.

We will look at several case studies through this lens of street--level algorithms.
First, we will discuss how YouTube's monetization algorithms targeted LGBTQ content creators.
Second,
we will analyze the algorithmic management of crowd workers,
discussing problems that arise when algorithmic systems evaluate the quality of workers
who must themselves make interpretations about underspecified tasks.
Third, we will look at judicial bail--recommendation systems and interrogate their biases
--- for example, disproportionately recommending jail for people of color.
In all of these cases,
we'll illustrate the difference between policy explication and policy execution, or
in some other sense, the difference between training data and test data.


\topic{We will discuss ways that designers of sociotechnical systems can mitigate the damage that street--level algorithms do in marginal cases.}
Drawing on analogy to street--level bureaucracies, we identify opportunities for designers of algorithmic systems to incorporate mechanisms for recourse in the case of mistakes and for self-policing and audits.
Recognizing that bureaucrats often operate by precedent, we suggest that
people might compare their case to similar cases seen by the algorithm,
arguing specifically how their case is marginal relative to previously observed cases.
% For example,
% LGBTQ content creators on YouTube could appeal on the grounds that
% the examples provided by the algorithm were all sexual, and that
% their videos were categorically different:
% about gender, but not sexuality; or a discussion on sexuality, rather than an instance of sexuality.
%If the algorithm operated on a decision boundary and was unaware of the marginality or dimensionality of the situation,
%this appeal makes clearer how and why the algorithm should be retrained.
We also reflect on the emerging set of cases where street--level bureaucrats utilize or contest the output of street--level algorithms, for example judges making decisions based in part on the predictions of bail-setting algorithms.

We suggest that many of today's discussions of the invisible systems in human--computer interaction might also be considered interrogations of street--level algorithms.
When we talk about systems that decide which social media posts we see~\cite{Eslami:2015:IAA:2702123.2702556},
when we find ourselves in conflict with Wikipedia bots~\cite{geiger2018lives,Geiger:2013:LBW:2491055.2491061},
and when we enforce screen time with our children~\cite{Hiniker:2016:STT:2858036.2858278},
we are creating, debating, and working around street--level algorithms.
Our hope is that the lens we introduce in this paper can help make sense of these previously disconnected phenomena.


% \onlyinsubfile{
%   \bibliographystyle{ACM-Reference-Format}
%   \bibliography{references}
% }
\end{document}
