\documentclass[main]{subfiles}

\begin{document}
\section{Introduction}


% \onlyinsubfile{

revising vs reinforcing?

algorithms don't \textbf{reflect} on the marginality of the cases they see.
it doesn't understand the concept of the margin

\begin{enumerate}
  \item (A) {Digital systems mediate a lot of our lives that seem counterintuitive and contrary to our expectations} (phenom).
      (dependencies: none)

  \item (B) {Researchers have started thinking about these issues from various angles: the hegemonic power these systems have, the unfair and sometimes opaque power they weild over us, and the confusion of metrics for goals.} (fatml/hci/comm/etc) but even if we did all these things right (designed inclusively, etc...), some agent would always have to make a decision about cases that it's never seen before (and will inevitably make a decision that's incorrect or counterintuitive ) (include ``fill in the gaps'')
      (dependencies: A)
  
  \item (C) {bureaucratic theory talks about street--level bureaucracies, the layer of bureaucratic organizations where people ``fill in the gaps'' --- making crucial decisions about cases that haven't been spelled out already.} (specifying the edges where points don`t exist')
      (dependencies: B (``filling in the gaps''==``making decisions about novel cases''))

  \item (D) These decisions are enormously consequential, because they effectively \textbf{become} policy
      (dependencies: C (filling in the gaps=>making policy))
  
  \item (E) there are also ``street--level algorithms''. (also why relevant to HCI) (policy => execution; training data => testing data)
      (dependencies: C + A (``street--level''=>C, ``algorithms'' => A))
  
  \item (F) what's different about the two is how they specify the edges (algo pretrains and executes at scale (or not idc); SLBs find that policy as they go, and refine over time) (dynamically reconfiguring the edges they draw (maybe more like making sense of the thing they're seeing? )) (msb: execution is also articulation for the executor of the thing --- when someone makes a decision about something, they're also articulating that \textbf{to themselves}; for an algo, articulation precedes execution) (in other words, people think about the decision they made (and why) after they've made it --- algorithms don't (they just execute after they've been trained) )
      (dependencies: E, (need to explain that SLB revision is a new idea))
  
  \item (G) bureaucracies become self--correcting (or at least aware of the decision that needs to be made (\textbf{and scales?})); algos does the same but without the awareness of policy being made (either at small or large scales)
      (dependencies: F + D)


A->B->C->D-----\
|     |---->E->F->G
|---------/
  \par\noindent\rule{\textwidth}{0.5pt}

% \hline

  \item people and algorithms fill in gaps in different ways; how does this lead to different outcomes that people have been seeing in the beginning?

  \item something about SLAs is different from SLBs in an important way that affects the outcome


  Bureaucrats fill in the gaps by reflecting on the underlying goals of their task

  msb: what's interesting isn't how they fill in gaps that are well-circumscribed, but what they do with the ambiguous cases

  ali: what's interesting to us isn't that they're exercising discretion (but hey, that's important too), but what motivates that (or something)
  % between prescribed behavior which can only be loosely prescribed in cases involving people. (todo: describe filling in the gaps as \textit{specifying the edges}, not filling the interior)


  \item {In this paper, we'll show that algorithmic systems are taking up the roles of street--level bureaucrats, becoming ``street--level algorithms'', and show how this way of thinking about these systems can help inform the design of these systems.}

  % \par\noindent\rule{\textwidth}{0.5pt}

  \item \thesis{``Street--level algorithms'' make decisions about myriad cases just like street--level bureaucrats, but unlike bureaucrats their decisions don't continually inform the ongoing revision of policy that bureaucrats engage in.}


  % \item we'll show how algorithmic systems ``fill in the gaps'' in marginal cases, sometimes classifying similar cases differently, but never using those cases, which we'll call ``salient cases'' for now, to prompt reflection on the decisions they make.
  % \item We'll discuss some of the ways that designers of sociotechnical systems can mitigate some of the shortcomings in algorithmic systems today.
  % \item This argument isn't gripping me. \ali{The argument that they should be reflecting on the marginality of the case and learning and changing over time would make more sense if this was a decades--long thing (maybe some really advanced AI that was trained in the 50s that didn't mature along with society, for instance). Instead what's happening is coming quickly. The problem seems to stem from not understanding the intuition that defines the nuance at these margins, and these systems aren't incentivized to make risky guesses. Does this just mean ``more data'' a sufficient answer to this problem? Because if so, ???}
  
  % \par\noindent\rule{\textwidth}{0.5pt}\setcounter{enumi}{6}
  % \item \thesis{``Street--level algorithms'' have become the focus of so much frustration in the public --- and the focus of so much attention and reasoning in the academic discourse --- because \textbf{they don't act as advocates for the people they serve}.} %, both in identifying whom to advocate for and how to advocate for them.}
  
  % \par\noindent\rule{\textwidth}{0.5pt}\setcounter{enumi}{6}
  % \item \thesis{``Street--level algorithms'' have proven so frustrating because, when they intuit appropriate decisions to ``fill in the gaps'', \textbf{they effect policy without reflecting on the consequences} of those decisions, and consequently \dots (???)}
  
  % \par\noindent\rule{\textwidth}{0.5pt}\setcounter{enumi}{6}
  % \item \thesis{``Street--level algorithms'' make the wrong decisions in marginal cases precisely because they don't understand their task, let alone underlying goals, and inevitably \textbf{they make decisions that advance their tasks, but not their goals}.}

  % \par\noindent\rule{\textwidth}{0.5pt}\setcounter{enumi}{6}
  % \item \thesis{``Street--level algorithms'' don't effectively parse out the difference between marginal and non--marginal cases. As a result, cases at the margins don't prompt SLAs to reflect on the decision space that they're operating in.}

  % \par\noindent\rule{\textwidth}{0.5pt}\setcounter{enumi}{6}
  % \item \thesis{``Street--level algorithms'' are doing great actually everything's fine relatively speaking keep up the good work}

\end{enumerate}
% }




\end{document}