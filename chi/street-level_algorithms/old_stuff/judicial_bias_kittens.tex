\onlyinsubfile{
  \subsection{\ali{what would have happened if the system had some self--awareness (i.e. what if the AI were like \textit{actually} intelligent?)}}
}

\topic{A self--aware agent tasked with recommending (or setting) bail for defendants would bring to the table knowledge about the circumstances surrounding that decision, and take account of reasons \textit{not} to press the weight of the court on a defendant.}
A good judge might recognize, for instance, that a marginal speeding ticket or similar infraction does little to advance the goal of the court to make the public safer; consequently, they might throw out frivolous, but valid, cases.
In the case of bail--setting in particular, we would hope that a judge would take into account the documented tendency of people unable to raise bail (and thus stuck in jail awaiting trial) to plead guilty, and make a more conscious effort to right that wrong.
In these sorts of cases that are not prescribed by judicial guidelines or clear precedent, well--informed, thoughtful judges are supposed to be able to reflect on their role in the court and their capacity to achieve \textit{justice},
even if it means not processing as many trials, which may be one of the tasks by which they're otherwise evaluated.

% A system effectively (if not also formally) making decisions about bail ought to be aware of the underlying history that has led
% to the court as it is today as well as
% to the defendant's presence in the court --- and not just their personal history, but often the history of that group of people and their relationship with the criminal justice system.
% Understanding the nature of those relationships is critical to making informed, well--reasoned decisions.

\onlyinsubfile{
  \subsection{\ali{how do we make a stopgap to that end? (should be bureaucratically informed!)}}
}

\topic{Algorithmic bail recommendation systems will remain problematic as long as the reasoning behind the recommendations themselves remains opaque.}




% \noalign\hline{\textwidth}


% \subsubsection{\ali{agnostic?} description of what happens}

% \topic{Engineers promised that algorithmic systems would make criminal justice fairer and more efficient, but by all accounts it seems to have replicated racial biases and further entrenched prejudices and injustices.}
% Systems across the United States make recommendations regarding whether to allow a defendant to walk free in the days leading up to a trial ---
% a decision judges have historically had to make without the benefit of copious amounts of data on outcomes from previous cases and complex modeling technology.

% But what judges did have was contextual and historical familiarity with the limits of the system and what can be documented.
% Humans in the American criminal justice system tend to know about the nuanced, generally racist history of the prison system to which judges continue to send convicts.
% Bearing that in mind, humans are cap
% In the United States, judges have struggled for decades to determine appropriate, ethical amounts for defendants to post bail in advance of trial.
% Algorithmic bail--setting promised to alleviate uncertainty for judges by incorporating enormous amounts of structured data and
% returning recommendations as to whether a defendant should be allowed to go free.
% But these systems appear to have recreated many of the biases that we see in the criminal justice system ---
% for instance, a preference for jailing African Americans, or for letting white people go free.



% align he street level bureaucrats and make it clear why/how slbs MAKE policies (do it better)

% what's the insight that we bring to the fact that biases exist in bureaucracies? what do they do that deals with that fact?

% legal language for law as written vs law as interpreted


% In the United States, judges in criminal cases have authority to set something called ``bail'', an amount of money that defendants can pay to the court to hold until they return for their court date. If the defendant appears for trial, their bail is returned to them. It affords those with sufficient financial means to retain autonomy during a trial, being allowed to go home and spend time with family for instance, rather than spend that time in jail.

% But setting bail has always been tricky; in an economic, game--theory sense, the optimal bail rates would be sufficiently low that every defendant would be able to pay it, but sufficiently high that no defendant would miss their court dates (or flee the court's jurisdiction) for fear of losing too much money.
% In practice, judges sometimes set bail too high, making it too hard for defendants to raise the necessary money.

% It's important to note that \textit{not} being able to pay bail, and spending time in jail awaiting trial, is more than just an inconvenience;
% people in jail are more likely to plead guilty to any given charge, suggesting that prosecutors have leverage over defendants if they an convince a judge to set bail too high for the defendant to materialize.
% If prosecutors are effectively coercing defendants into harsher sentences than the evidence should warrant in a fully drawn out trial with jurors, then arguably bail is being misused.


% So there's some sort of function here that says you want to set the lowest possible bail that will ensure people return for their court date. Given information about defendants such as their income, assets, relationships with people outside of the court's jurisdiction, etc\dots computer scientists have hypothesized that this may be a problem that is, in the computational sense, solvable.

% So computer scientists have begun to explore this problem, and computational social scientists have identified that there are heuristics that we can apply that get us something like 80\% or 90\% of the way toward maximizing the number of people who return for their court date by implementing a scoring system (e.g. ``if the defendant has been arrested multiple times before this appearance, subtract 3 points; etc\dots now if the total score is above 15, then allow them to go free; if it's below 0, don't offer bail at all'' \ali{or something}).

% But these systems have picked up biases of their own
% % --- specifically in the form of biases against women and people of color --- resulting in
% uneven, and importantly systematically unfair, bail recommendations.
% \ali{this is awful but it's already too long.}

% But these algorithms make biased decisions of their own;
% Reporters at ProPublica have found that algorithmic systems have picked up biases that specifically disenfranchise women and people of color.
% As the public has come to terms with the nature and extent of the biases that increasingly inform decisions in courts, we've had to 
% about offenders have produced results biased against people of color and women.



% \vspace{10mm}

% Judges struggle to come up with appropriate levels for bail, a bribe defendants have to pay before they're even found guilty, to be allowed to go home rather than spend time in jail where they get strong--armed and coerced into pleading guilty to charges for which they're probably not guilty because the prosecutors have enormous leverage over defendants by threatening inflated charges.

% The prison system is a fucking scam. Abolish prison.


% Courts have begun using algorithmic systems to inform bail--setting norms by predicting the likelihood that a defendant will return for a scheduled court date.
% These systems are designed to take information such as the nature of the charges, the history of the accused, the means they have at their disposal to flee the jurisdiction of the court, and other factors to inform whether a given individual is likely to return.
% But these systems haven't taken into account numerous factors, such as the nature of prison systems and its bias against people of color;
% % as a result, they often make decisions that are oblivious to social issues like race, and make decisions that 
% as a result, these systems have made numerous decisions informed by the data but not by underlying sociological considerations that human judges consider.
% These systems, which promised to eliminate uncertainty about whether people would return to their court dates, have arguably erred on a side of caution that results in more white defendants being released on their own recognizance than black defendants, \ali{other salient examples?}


% what's the technique that's getting applied here?
% in feminist HCI: the question is "who's being minimized/marginalized here?"


% what's the question in slbs? what's the difference between the values described vs the values instantiated? where does that happen? why and how does that happen?

\begin{comment}
\subsubsection{\SLBs take}

\topic{If we think about these bail--setting recommendation systems as \slbs that work alongside judges, clerks, and others, the roles of these systems come into sharper focus.}
% talk about the role of the system
Prosecutors make decisions that are largely up to their discretion, such as
what charges to bring to a defendant whose alleged actions might correspond to a number of crimes.
These choices are important because they largely determine the potential range of outcomes (regardless of a guilty or not guilty verdict).
Judges, for their parts, make decisions about
whether to accept certain charges made by the prosecutor,
how to conduct the trial, and myriad other aspects of the events within the court.
Again, these decisions matter enormously, as they effectively mediate the journey toward the court's verdict.
Systems that make recommendations about whether to let a defendant out on bail or not
work in concert with the input of prosecutors, the record of the defendant, and other agents of the court
to make recommendations.

\topic{The decisions these systems make themselves become policies.}
These systems, like a judge who prefers to give lenient sentences to white defendants, or
a prosecutor who threatens harsher charges on people of color,
risk becoming engines that motivate discriminatory, prejudicial behavior that becomes \textit{de facto} policy for the jurisdiction in which that system is used.
In much the same way that New Jersey has decided, bureaucratically, not to charge marijuana--related crimes until September of 2018,
algorithmic bail recommendation systems can (and do)
make systematic decisions that become systemic, and ultimately become policy almost as firmly as formal policy or legislation itself.

\topic{As \slbs, algorithmic bail--setting recommendation systems have been instructed to optimize for one goal, and in doing so run roughshod over other nuanced goals that judges tend not.}
% talk about the goal being to optimize for return rates, not for freedom or other values that are hard to encode
\textit{\citeauthor{bowker2000sorting} and others have written about categorizing things;
I know of some stuff from sociology literature (all from Quantified Self stuff, but critical and relevant).
that stuff, in here}

\topic{Further, bail--setting recommendation systems learn from data which is structured and, critically, constructed; this leads to biases in line with the bias that informed the collection of data itself.}
{Machine Learning critiques here. none of this is exciting new stuff to me; it's the scholarship we all know and love}

\topic{Together, the motivated collection of data and the goal--setting of the algorithm itself create circumstances familiar to bureaucratic researchers.}
{lemme tell you about the Tennessee Valley Authority\dots}


\topic{How courts in particular have dealt with these issues is materially interesting to us, and where we take some advice.}
{We vote for representatives who carefully select judges for their discretion \textit{rather than} for partisan views.
indeed, the current events that make SCOTUS nominations seem so bizarre precisely because judicial nominations don't reflect the pattern we're used to seeing:
the nomination of dedicated bureaucrats whose decisions are motivated by practical discretion rather than by political ideology.}
% what are the corresponding in slbs and bureaucracies? how do bureaucrats deal with this?

\topic{It's critical that we begin to think of algorithmic systems in roles such as these as the result of choices made by their designers,
in very much the same way that we think of judges as the selections of the politicians who nominate them.}
% what good could come here? what dangers are there?

\ali{There's so much more to say here, like about Jim Crow laws and how the courts and judges in particular made these things real and not just words on paper.}



% We should stop thinking about algorithmic recommendation systems as perfect actors, with no agendas, making objective calculations.
% Algorithms here, as in the other case studies (and, honestly, like, everywhere), are
% the products of people who themselves have goals and wish to make those goals real.
% Politicians direct organizations to fill roles with bureaucrats and to staff the many positions of \slbs;
% those people that become street--level bureaucrats take their sense of the goals of the organization and make those values a reality.

% Street--level bureaucrats make complex decisions as experts in their areas with little oversight or scrutiny by nature of the work they do; algorithmic systems work the same way, brought into existence by the political actors who design them.




% \footnotemark.

% \footnotetext{Whether software engineers are cognizant of the political nature of the decisions they make --- from the design of machine learning systems to the sources and dimensions of data that they use to inform these systems --- is irrelevant; these decisions are being made in a context from which social contexts can't be extracted.\footnotemark.}

% \footnotetext{If I need to cite something about this I'll cite Foucault about counter--culture or language or something (maybe it was Chomsky) being always defined in relation to ``culture''.}

% We should think about these algorithmic systems as agents in the same ways that we think about judges;
% similarly, we should think about the motivations, views, and incentives that we give people who \textit{design} these systems,
% just as we think deeply about the politicians who select, nominate, and appoint judges.

% But more importantly, since we can now ``design'' entities which make
% influential recommendations toward decisions that judges have historically made alone,
% we should start thinking more about what \textit{values} we want to encode in these agents.
% We can, for instance, consciously encode into these systems some accounting of systemic racial injustices
% that have, by many accounts \ali{citations here},
% precipitated a powerful racial bias in the prison system itself.

% Recommendation systems can't
% undo centuries of systematized violence against people of color, nor can they necessarily
% negate the intent among people to exercise prejudicial bias, but
% it can (we hope) make recommendations that are not completely oblivious to racial biases, and thus blithely recreating them.

% \footnote{\ali{This is a ramble that goes nowhere but it might help clue you into the kinda foggy thinking that's plaguing me}

% I have mixed feelings here. on one hand, bureaucratic organizations can make policies without the slow deliberative process of legislation, and justice (according to the public, from whom bureaucrats often come\dots) can get instantiated just by virtue of the people in the system exercising their discretion collectively in a way that serves the public.

% On the other hand, \slbs are collective because there are as many people providing input and determining policy as there are bureaucrats;
% in ``street--level algorithms'' everyone is essentially an agent of the person (or people) who created them.
% The perspectives of vanishingly smaller groups of people, often privileged in myriad ways, become the informing perspectives that decide policies like what makes a person ``a danger to society'', for instance.

% I don't have any good answers to this. It just seems like a really bizarre disconnect and I can't close this thread.
% More people can code, and that's good, but empirically tech companies are overwhelmingly male and white.
% Are bureaucracies any more accessible for people of color? (have they historically been? this one's rhetorical; the answer's no)}


% Judges are street--level bureaucrats;
% they make enormously consequential decisions about how to apply regulations and laws to myriad situations, with oversight almost exclusively from other (namely, higher) courts



% % Evidence on judicial discretion illustrates reasonably clearly that human 
% Algorithmic judicial discretion resembles human discretion perhaps more clearly than any of the other examples in part because of the nature of the failures that emerge;
% algorithms trained on several quantified dimensions of data with no underlying awareness or appreciation of
% \begin{enumerate*}
%   \item the value of judicial discretion and compassion, or
%   \item the complicated, but deeply important racial history that courts have had with people of color
% \end{enumerate*}
% ultimately replicate the very bias that generated that data --- that is, they make decisions that reinforce and codify policies of racism

\end{comment}
