\onlyinsubfile{
  \subsection{\ali{what would have happened if the system had some self--awareness (i.e. what if the AI were like \textit{actually} intelligent?)}}
}

\topic{In a situation like this, \citeauthor{lipsky1983street}'s observation that people need to have credible \textit{hope} that their appeal will be favored seems especially relevant.}
People need to believe that their appeals will be heard by an agent that will reflect on the nuance and uniqueness of the work in question.
A good street--level bureaucrat would look at the work in all of its dimensions and reflect on the underlying goal --- which may, in this case for instance, be to promote and encourage managed creative exploration.
More important than the exact characteristics of the street--level bureaucrat would be the trust among workers that, if their work is rejected, they can appeal it and have reasonable hope of a good outcome.
That would mean an algorithmic system that substantively reflects on the effort that the worker put into the microtask,
thinks about the purpose of classifying the worker's output as correct or incorrect, and comes to a nuanced conclusion.
For instance, this might be a conclusion about the \textit{intent} of the worker, and a decision about whether to provide a worker with more information or corrective guidance to do the task correctly, versus ``firing'' the worker.


\onlyinsubfile{
  \subsection{\ali{how do we make a stopgap to that end? (should be bureaucratically informed!)}}
}

\topic{In the meantime, some mechanism for workers to explain the marginality that leads to a human judge would give crowdworkers some sort of backstop on the pressure to submit predictable, uncreative work.}
This could manifest in a few ways.
One option would be to classify rejection reasons for workers and provide them with some insight into why their work was rejected, and subsequently providing them with a mechanism to appeal the rejection.
That appeal system would have to go to a human, or at least an independently designed and trained classifier than the one that made the original decision.
This wouldn't be a panacea to many of the problems that microtask employers experience, but it would serve as a stopgap for some of the pressures that discourage workers from engaging in creative work that might be algorithmically dismissed out of hand.
% By providing workers with some sort of guarantee that their work --- even ``risky'' or creatively ambitious work --- will be evaluated favorably in a broader context, workers will be able to take on riskier work that they otherwise would assume 
% Without that hope, the only people who will engage with the bureaucracy (and thus, the street--level bureaucrats) will eventually become more risk--averse workers who are confident that their work satisfies the criteria laid out.



\begin{comment}
\topic{On--demand workers in particular have been seeing the forefront of algorithmic management for the better part of a decade.}
Systems such as Uber, Lyft, and Amazon Mechancial Turk (AMT) digitally mediate the experiences of these transient workers in myriad ways:
Who's eligible for work?
Who should be offered a new job first?
How long should these people have to accept or reject the job?
How much information should be revealed to the worker before the job has been accepted?

These questions and others are part of the negotiation that workers engage in with computational systems,
which have grown increasingly complex but
which fundamentally serve roles that resemble those of car dispatchers (taxi services) or hiring hall administrators (miscellaneous work).

% \subsubsection{\SLBs take}
When viewed through the lens of \slbs, we start to see that systems like Uber and Lyft,
which fashion themselves as agnostic platforms upon which the ``free market'' operates,
are in fact actively involved agents, and themselves street--level bureaucrats, or rather ``street--level algorithms''.

These systems should be acknowledged for the work that they do, but similarly
they should be demandingly scrutinized, and we should reflect on the values that these systems encode.
\ali{If Uber's system knows that a passenger will cancel a ride if the driver happens to be African American,
what should the system do\footnotemark?}

\footnotetext{This kind of conversation would arguably be reasonable to have if all of the agents were human beings
(something like ``well the bureaucrat should just tell the customer to go away'' or something), but
it seems to be more taboo (or at least more difficult) to talk about what Uber should tell its algorithm to do about the same interaction and exercise.}


\topic{So let's pull the literature from \slbs back in;
what they offer in situations like this is that bureaucrats
--- the agents that make important decisions about where to send people for jobs, what jobs to offer them, etc\dots ---
have historically made those decisions cognizant of the needs of drivers, and of workers' circumstances.}
When hiring hall administrators determined worker eligibility and priority,
they prioritized workers in need --- a decision that was made in conversation with workers.
% they were ultimately accountable to workers in very real ways.

\SLBs should operate the same way

\end{comment}
