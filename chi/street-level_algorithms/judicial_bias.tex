%!TEX root = ./main.tex
\documentclass[main]{subfiles}
\begin{document}

\subsection{Algorithmic bias in justice}

\topic{American courts have, in recent years,
turned to algorithmic systems to predict
whether a defendant is likely to appear at a subsequent court date,
recommending the level at which bail should be set.}
The idea is to train these systems based on
public data such as whether historical defendants who were let out on bail
actually showed up at their court date.
These systems take into account dimensions such as
the charges being levied,
the defendant's history,
their income level, and
much more, in the hopes of yielding outcomes that increase public welfare,
for example by reducing jailing rates by 40\% with no change in resulting crime rates~\cite{kleinberg2017human},
all while being less biased and more empirically grounded.

Instead, observers have seen patterns of bias that
either reflect or amplify
well--documented prejudices in the criminal justice system.
Researchers have found bail--recommendation systems replicating and exacerbating
racial and gender biases
--- recommending against offering bail to black men disproportionately more than for white men, for example~\cite{propublica}.
In some cases,
it seems that problems stem from the data that informs models~\cite{buolamwini2018gender};
in others, recommendation systems are,
as AI researchers say,
reflecting a mirror back at our own society,
itself steeped in racial prejudice~\cite{lambrecht2018algorithmic,thebault2015avoiding}.

\topic{In this case, the analogical street--level bureaucrat is probably clear. It is the person whose work the algorithm seeks to replicate: the judge.}
These algorithms are often even trained on judges' prior decisions~\cite{kleinberg2017human}.
However, as street--level bureaucrats, judges have struggled to answer the question of
``which defendants secure release before trial?'' for most of the \nth{20} century~\cite{walker1993taming}.
While constitutional law protects people from ``excessive bail'',
\citeauthor{walker1993taming} points out that ultimately
this decision is left to the discretion of the judge~\cite{walker1993taming}.
A judge hears preliminary information about the case,
reviews information about the defendant
(such as past criminal record, assets, and access to means of travel), and
sets bail that should be sufficiently high that a defendant will appear for their court date without being inaccessible.

In this third case study, we observe something new: a street--level bureaucrat interacting with a street--level algorithm.
This interaction can be fraught:
bureaucrats in the judicial system resist, buffer, and circumvent
the algorithmic recommendations, especially as those algorithms attempt to subsume the work of those bureaucrats.
Indeed, \citeauthor{doi:10.1177/2053951717718855} explores some of the tensions that emerge
when algorithms begin to absorb bureaucrats' responsibilities and shift the latitude that bureaucrats enjoyed,
finding that bureaucrats work around and subvert these systems through
foot-dragging, gaming, and open critique as a way of keeping their autonomy~\cite{doi:10.1177/2053951717718855}.
\citeauthor{Veale:2018:FAD:3173574.3174014} go further to illustrate some of the ways that
designers of algorithmic systems can better support
street--level bureaucrats given these and other tensions~\cite{Veale:2018:FAD:3173574.3174014}.

\topic{Researchers have contributed many valuable insights about
bail recommendation algorithms from the perspective of fairness, accountability and transparency
(reviewed in~\cite{corbett2018measure});
the literature of street--level bureaucracies adds a reminder that each case may involve
novel circumstances and deserves thoughtful consideration about which humans in particular are well--equipped to reason.}
As \citeauthor{lipsky1983street} writes,
``street--level bureaucrats~\dots~\textit{at least [have] to be open to the possibility}
that each client presents special circumstances and opportunities
that may require fresh thinking and flexible action.''~\cite{lipsky1983street}.
Otherwise, why bother having judges or trials at all?
Why not articulate the consequences directly in the law,
feed the circumstances of the crime into a predefined legal ruleset
(e.g., \texttt{\{crime: murder\}, \{eyewitness: true\}, \{fingerprints: true\}}), and
assign whatever conclusion the law's prescriptions yield?
Largely the reason that our society insists on the right to a trial is that
there may be relevant characteristics that cannot be readily encoded or have not been foreseen in advance.

\topic{If street--level algorithms are liable to make errors in marginal and novel situations,
it suggests that the problem is not just how to handle biased data, but also
how to handle missing data.}
Increased training data is insufficient: for important cases at the margin,
there may be no prior cases.
Intersectionality is growing as an area of focus within HCI~\cite{schlesinger2017intersectional};
intersectionality fundamentally calls attention to the fact that combinations of traits
(e.g., being a woman and a person of color)
need to be treated as a holistically unique constellation of traits, rather than
as some sort of sum of the individual traits.
As a matter of probability,
each additional dimension in the intersection makes that constellation less likely.
While similar cases in the mainstream may well have been seen before by the algorithm,
when the case is at the margin,
its particular intersection of traits may be completely novel.
Adding training data is almost a waste of resources here, as
the combination may be so rare that even
increasing dataset size tenfold or one hundredfold
may only add a single additional instance of that combination.

\topic{In practice,
this intersectional challenge is one reason why many democracies use a form of case law,
allowing an individual to argue to a judge that
their circumstances are unique and should be examined uniquely, and
with discretion.}
Many cases are straightforward; however, when they're not,
the court system must re--examine the case and the law in this new light.
How could an algorithm identify a situation that needs to be treated as
a novel interpretation of a policy, as opposed to
one that is only a small variation on a theme that has been seen before?

\topic{Much of the discussion of
judicial bail recommendation algorithms today is focused on
the goals of fairness, accountability and transparency, or \textit{FAT}.}
We argue that FAT is a necessary, but not sufficient, goal.
Even a perfectly fair, transparent, and accountable algorithm
will make errors of generalization in marginal or new cases.
% Algorithmic systems don't reflect on
% the overall role that they play in the criminal justice system
% when they make bail recommendations.
% \msb{That last sentence comes out of nowhere and makes a new argument.
% Reflecting on your overall role is related to, but different than,
% reflexivity in interpreting policy. Can we cut that last sentence?}
% \ali{Yeah I don't feel super strongly about it}

\onlyinsubfile{
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{references}
}
\end{document}
